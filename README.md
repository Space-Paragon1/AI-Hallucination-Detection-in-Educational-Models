# Hallucination Guard for Educational Math Models

**Hallucination Guard** is a research-oriented system for **detecting, explaining, and mitigating hallucinations in AI tutoring models**, with an initial focus on **Algebra** and a planned extension to **Calculus**.

This project is part of a broader **AI + Education + Systems** research portfolio.  
The core idea is to place a *verification and risk-assessment layer* between an educational AI model and a student, reducing the likelihood that **confident but incorrect mathematical answers** are presented as truth.

---

## ðŸš© Problem Motivation

Educational AI systems can produce answers that are:
- Fluent and confident
- Step-by-step and pedagogical
- **Mathematically incorrect**

In learning contexts, these hallucinations are especially harmful because:
- Students may trust incorrect reasoning
- Errors can compound over time
- Models often fail *silently* without expressing uncertainty

This project addresses that gap by detecting hallucination risk **before** an answer reaches the learner.

---

## ðŸŽ¯ Project Goals

- Detect hallucinated math answers using **verifiable signals**
- Assign a **calibrated risk score** (0â€“1)
- Provide **human-interpretable explanations** for why an answer is risky
- Enforce **policies** (allow / ask / block) appropriate for educational settings
- Build a system that scales cleanly from Algebra â†’ Calculus â†’ other STEM domains

---

## ðŸ§± System Architecture (High Level)

Question + Model Answer
â”‚
â–¼
Feature Extraction (textual + math signals)
â”‚
â–¼
Math Verifiers (Algebra now, Calculus later)
â”‚
â–¼
Risk Model (calibrated classifier)
â”‚
â–¼
Policy Engine (allow / clarify / block)
â”‚
â–¼
Student-Safe Response


---

## ðŸ“ Phase 1A: Algebra Hallucination Detection (Current)

**Scope**
- Linear equations
- Basic algebraic manipulation
- Arithmetic and reasoning errors

**Key Techniques**
- Numeric answer extraction
- Equation plug-in verification
- Heuristic + ML-based risk estimation
- Controlled error dataset construction

**Outputs**
- `risk`: probability of hallucination
- `label`: low / medium / high risk
- `action`: allow / ask_clarifying / block_and_verify
- `reasons`: interpretable explanations

---

## ðŸ“Š Dataset Strategy

There is no off-the-shelf dataset for *educational hallucinations*.

Instead, the dataset is constructed via **controlled error generation**:

- ~100 algebra questions
- For each question:
  - 1 correct solution
  - 2 plausible but incorrect solutions
- Total: **300+ labeled examples**

Each example is stored in JSONL format with:
- hallucination label
- error type
- severity level

---

## ðŸ—‚ Repository Structure

hallucination-guard-math/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI entrypoint
â”‚ â”‚ â”œâ”€â”€ routers/ # API endpoints
â”‚ â”‚ â”œâ”€â”€ core/ # schemas, scoring, model loading
â”‚ â”‚ â”œâ”€â”€ features/ # algebra feature extraction
â”‚ â”‚ â”œâ”€â”€ verifiers/ # math verifiers (algebra, calculus stubs)
â”‚ â”‚ â”œâ”€â”€ policy/ # risk â†’ action logic
â”‚ â”‚ â””â”€â”€ data/ # train/dev datasets
â”‚ â””â”€â”€ models/ # trained model artifacts
â”œâ”€â”€ experiments/
â”‚ â”œâ”€â”€ train.py # model training
â”‚ â””â”€â”€ eval.py # evaluation scripts
â”œâ”€â”€ docs/
â””â”€â”€ README.md


---

## ðŸš€ Running the Project

### Requirements
- **Python 3.10+**
- Windows / macOS / Linux

### Setup
```bash
python -m venv .venv
source .venv/bin/activate  # or .\.venv\Scripts\Activate.ps1 on Windows
pip install -r backend/requirements.txt

Start the API
From the repo root: uvicorn backend.app.main:app --reload

Useful Endpoints
Health check:
http://127.0.0.1:8000/health

Interactive API docs:
http://127.0.0.1:8000/docs

Score an answer:
POST /score

ðŸ§ª Example Request
{
  "question": "Solve for x: 2x + 5 = 17",
  "model_answer": "2x + 5 = 17 -> 2x = 10 -> x = 5",
  "student_level": "Algebra I"
}

Example Response
{
  "risk": 0.91,
  "label": "high_risk",
  "action": "block_and_verify",
  "reasons": [
    "Final answer does not satisfy the equation when plugged in."
  ]
}

ðŸ”¬ Evaluation Metrics
AUROC (hallucination detection)
F1 score
Calibration quality (risk â‰ˆ true error rate)
False-positive cost (blocking correct answers)
False-negative cost (allowing hallucinations)

ðŸ”® Planned Extensions (Phase 1B & Beyond)
Symbolic verification using SymPy
Derivative and integral checking (Calculus)
Limit evaluation (symbolic + numeric)
Conceptual explanation quality analysis
Adaptive tutoring policies based on student level
Expansion to Chemistry and Physics domains

ðŸ“Œ Research Positioning
This project sits at the intersection of:
AI Safety
Educational Technology
ML Systems
Human-Centered AI
It is designed to be:
Reproducible
Interpretable
Extensible
Grounded in verifiable mathematics

THIS PROJECT IS STILL IN PROGRESS.
